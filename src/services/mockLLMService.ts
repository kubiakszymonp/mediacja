import { LLMResponse } from './llmService';

export interface Message {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export class MockLLMService {
  public async sendMessage(messages: Message[]): Promise<LLMResponse> {
    try {
      // Logujemy wszystkie wiadomo≈õci przychodzƒÖce
      console.group('üì• Otrzymane wiadomo≈õci:');
      messages.forEach((msg, index) => {
        console.log(`[${msg.role.toUpperCase()}] ${index + 1}:`, msg.content);
      });
      console.groupEnd();

      // Symulujemy op√≥≈∫nienie sieciowe
      await new Promise(resolve => setTimeout(resolve, 1000));

      // Generujemy przyk≈ÇadowƒÖ odpowied≈∫
      const mockResponse: LLMResponse = {
        content: JSON.stringify([
          "To jest przyk≈Çadowy punkt podsumowania 1",
          "To jest przyk≈Çadowy punkt podsumowania 2",
          "To jest przyk≈Çadowy punkt podsumowania 3"
        ]),
        usage: {
          promptTokens: 100,
          completionTokens: 50,
          totalTokens: 150
        }
      };

      // Logujemy odpowied≈∫
      console.group('üì§ Wygenerowana odpowied≈∫:');
      console.log('Tre≈õƒá:', mockResponse.content);
      console.log('U≈ºycie token√≥w:', mockResponse.usage);
      console.groupEnd();

      return mockResponse;
    } catch (error) {
      console.error('‚ùå B≈ÇƒÖd w mock LLM:', error);
      throw error;
    }
  }
}

export const mockLLMService = new MockLLMService(); 